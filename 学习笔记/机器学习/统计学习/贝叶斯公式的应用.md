### 贝叶斯公式理解与应用

---

#### 一、什么是贝叶斯公式

​	认识贝叶斯公式之前，先了解一下条件概率，先验概率，后验概率的概念。

##### 0、举个栗子

​	假设某天小王出门堵车了，而小王想要分析这时候堵车的原因。由于堵车的因素有很多，这里我们假设出门堵车的可能因素有两个：车辆太多和交通事故。

##### 1、条件概率

- $P(A|B)$表示在B事件发生的情况下，A事件发生的概率。
- 其中定义一个事件$B=堵车$，一个大小为2的样本空间$A=｛车辆太多，交通事故｝$
- $P(A|B)=P(AB)/P(B)$表示在堵车事件发生下，导致今天这种堵车情况的原因的条件概率，有$P(交通事故|堵车)$，$P(车辆太多|堵车)$。

##### 2、先验概率

- 先验概率是指根据**以往经验和分析**得到的概率,如全概率公式,它往往作为"**由因求果**"问题中的"因"出现.
- 比如在抽取零件时，我们不知道能不能抽取到合格件，但是选择任意一个箱子的概率一定都为1/3，所以称为先验概率。
- $P(A)$表示${车辆太多，交通事故}$的发生可能性

##### 3、后验概率

- 后验概率是指在**得到“结果”的信息**后**重新修正**的概率，是“**执果寻因**”问题中的"果"。
- 后验概率的计算要以先验概率为基础。后验概率可以根据通过贝叶斯公式，用**先验概率**和**似然函数**计算出来

##### 4、贝叶斯公式


$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)} 【式1】
$$
把B展开，可以写成：
$$
P(A|B)=\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|∼A)P(∼A)} 【式2】（∼A表示”非A”）
$$
这里我们先了解

- $P（A|B）$就是后验概率，即B情况下，A发生的概率。


- $P（A）$是先验概率，即A本身发生的概率。通常，先验概率能从数据中直接分析得到，也可以通过经验得到，例如掷骰子1/6，掷硬币1/2。


- $P（B|A）$是似然函数（下面会介绍）


- 对于$P（B）$，  应该和实验主体有关，例如“投10次硬币”是一次实验，实验做了1000次，“反正正正正反正正正反”出现了n次，则$P(x0)=n/1000$。总之，这是一个可以由数据集得到的值

- 使用贝叶斯公式，为的是要计算最大后验概率估计（即最大$P(B|A)P(A$）），得到B情况下A（对于概率函数，A是变量，eg：车辆太多，交通事故）的最大可能性，最终得到的A（eg： 交通事故）就是我们要推出来的数据（原因）

  ​

**备注：**

- 注意区分条件概率和贝叶斯公式，可以说贝叶斯公式（后验概率）是一种条件概率



#### 二、为什么用贝叶斯公式

[这里引用某位网友的优质答案](https://blog.csdn.net/u011508640/article/details/72815981)

##### 0、举个栗子

​	一辆汽车（或者电瓶车）的警报响了，你通常是什么反应？有小偷？撞车了？ 不。。 你通常什么反应都没有。因为汽车警报响一响实在是太正常了！每天都要发生好多次。本来，汽车警报设置的功能是，出现了异常情况，需要人关注。然而，由于虚警实在是太多，人们渐渐不相信警报的功能了。

​	贝叶斯公式就是在描述，你有多大把握能相信一件证据？（how much you can trust the evidence）

​	我们假设响警报的目的就是想说汽车被砸了。把A计作“汽车被砸了”，B计作“警报响了”，带进贝叶斯公式里看。我们想求等式左边发生A|B的概率，这是在说警报响了，汽车也确实被砸了。汽车被砸引起（trigger）警报响，即B|A。但是，也有可能是汽车被小孩子皮球踢了一下、被行人碰了一下等其他原因（统统计作∼A），其他原因引起汽车警报响了，即B|∼A。那么，现在突然听见警报响了，这时汽车已经被砸了的概率是多少呢（这即是说，警报响这个证据有了，多大把握能相信它确实是在报警说汽车被砸了）？

​	想一想，应当这样来计算。用警报响起、汽车也被砸了这事件的数量，除以响警报事件的数量（这即【式1】）。进一步展开，即警报响起、汽车也被砸了的事件的数量，除以警报响起、汽车被砸了的事件数量加上警报响起、汽车没被砸的事件数量（这即【式2】）。

##### 1、优点1：考虑所有因素

​	再思考【式2】。想让$P(A|B)=1$，即警报响了，汽车一定被砸了，该怎么做呢？让$P(B|∼A)P(∼A)=0$即可。很容易想清楚，假若让$P(∼A)=0$，即杜绝了汽车被球踢、被行人碰到等等其他所有情况，那自然，警报响了，只剩下一种可能——汽车被砸了。这即是提高了响警报这个证据的说服力。

​	从这个角度总结贝叶斯公式：**做判断的时候，要考虑所有的因素**。 老板骂你，不一定是你把什么工作搞砸了，可能只是他今天出门前和太太吵了一架。

##### 2、优点2：谨慎对待每个因素

​	再思考【式2】。观察【式2】右边的分子，$P(B|A)$为汽车被砸后响警报的概率。姑且仍为这是1吧。但是，若$P(A)$很小，即汽车被砸的概率本身就很小，则$P(B|A)P(A)$仍然很小，即【式2】右边分子仍然很小，$P(A|B)$ 还是大不起来。 这里，P(A)P(A)即是常说的先验概率，如果A的先验概率很小，就算$P(B|A)$较大，可能A的后验概率$P(A|B)$还是不会大（假设$P(B|∼A)P(∼A)$不变的情况下）。

​	从这个角度思考贝叶斯公式：**一个本来就难以发生的事情，就算出现某个证据和他强烈相关，也要谨慎。证据很可能来自别的虽然不是很相关，但发生概率较高的事情。 **发现刚才写的代码编译报错，可是我今天状态特别好，这语言我也很熟悉，犯错的概率很低。因此觉得是编译器出错了。 ————别，还是先再检查下自己的代码吧。

#### 三、似然函数和概率函数

​	在认识似然函数和概率函数之前，先了解一下概率和统计的区别

[这里引用某位网友的优质答案](https://blog.csdn.net/u011508640/article/details/72815981)

##### 1、概率和统计

​	概率（probabilty）和统计（statistics）看似两个相近的概念，其实研究的问题刚好相反。

​	概率研究的问题是，**已知一个模型和参数，怎么去预测这个模型产生的结果的特性（例如均值，方差，协方差等等）。** 举个例子，我想研究怎么养猪（模型是猪），我选好了想养的品种、喂养方式、猪棚的设计等等（选择参数），我想知道我养出来的猪大概能有多肥，肉质怎么样（预测结果）。

​	统计研究的问题则相反。统计是，**有一堆数据，要利用这堆数据去预测模型和参数**。仍以猪为例。现在我买到了一堆肉，通过观察和判断，我确定这是猪肉（这就确定了模型。在实际研究中，也是通过观察数据推测模型是／像高斯分布的、指数分布的、拉普拉斯分布的等等），然后，可以进一步研究，判定这猪的品种、这是圈养猪还是跑山猪还是网易猪，等等（推测模型参数）。

​	一句话总结：**概率是已知模型和参数，推数据。统计是已知数据，推模型和参数**。

##### 2、似然函数和概率函数


$$
P(x|θ)
$$
​	输入有两个：$x$表示某一个具体的数据；$θ$表示模型的参数。

​	如果 $θ$ 是已知确定的，$x$ 是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点 $x$，其出现概率是多少。

​	如果 $x$ 是已知确定的，$θ$ 是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现 $x$ 这个样本点的概率是多少。

##### 3、MLE和MAP

[这里引用某位网友的优质答案](https://blog.csdn.net/u011508640/article/details/72815981)

​	最大似然估计（Maximum likelihood estimation, 简称MLE）和最大后验概率估计（Maximum a posteriori estimation, 简称MAP）是很常用的两种参数估计方法，如果不理解这两种方法的思路，很容易弄混它们。

###### 1）最大似然估计

​	假设有一个造币厂生产某种硬币，现在我们拿到了一枚这种硬币，想试试这硬币是不是均匀的。即想知道抛这枚硬币，正反面出现的概率（记为$θ$）各是多少？

​	这是一个统计问题，回想一下，解决统计问题需要什么？ 数据！

​	于是我们拿这枚硬币抛了10次，得到的数据（$x0$）是：反正正正正反正正正反。我们想求的正面概率θ是模型参数，而抛硬币模型我们可以假设是 二项分布。

​	那么，出现实验结果$x0$（即反正正正正反正正正反）的**似然函数**是多少呢？
$$
f(x0,θ)=(1−θ)×θ×θ×θ×θ×(1−θ)×θ×θ×θ×(1−θ)=θ^7(1−θ)^3=f(θ)
$$
​	最大似然估计就是**似然函数的最大值**。

​	[更多关于最大似然估计的图像分析，请看这里]((https://blog.csdn.net/u011508640/article/details/72815981))

​	一些人可能会说，硬币一般都是均匀的啊！ 就算你做实验发现结果是“反正正正正反正正正反”，我也不信θ=0.7。

​	这里就包含了贝叶斯学派的思想了——要考虑先验概率。 为此，引入了最大后验概率估计。

###### 2）最大后验概率估计

​	最大似然估计是求参数$θ$, 使似然函数$P(x0|θ)$最大。最大后验概率估计则是想求$θ$使$P(x0|θ)P(θ)$最大。求得的$θ$**不单单让似然函数大**，$θ$**自己出现的先验概率也得大**。 （这有点像正则化里加惩罚项的思想，不过正则化里是利用加法，而MAP里是利用乘法）

​	MAP其实是在最大化$P(θ|x0)=\frac{P(x0|θ)P(θ)}{P(x0)}$，不过因为$x0$是确定的（即投出的“反正正正正反正正正反”），$P(x0)$是一个**已知值**，所以去掉了分母$P(x0)$（假设“投10次硬币”是一次实验，实验做了1000次，“反正正正正反正正正反”出现了n次，则$P(x0)=n/1000$。总之，这是一个可以由数据集得到的值）。最大化$P(θ|x0)$的意义也很明确，$x0$已经出现了，要求$θ$取什么值使$P(θ|x0)$最大。顺带一提，$P(θ|x0)$即后验概率，这就是“最大后验概率估计”名字的由来。

[更多关于最大后验概率的图像分析，请看这里]((https://blog.csdn.net/u011508640/article/details/72815981))



#### 四、疑问解答

##### 1、问题1：后验概率和条件概率是一回事吗？

- 在贝叶斯公式里，后验概率就是一种条件概率。条件概率的概念更广泛，概率论，测度论，随机过程随机分析里用的概念都是条件概率。

##### 2、问题2：概率函数和似然函数是一回事吗？

- 不是，对于函数 $P(x|θ)$，若$θ$是已知确定的，$x$ 是变量，这个函数叫做概率函数，最大后验概率估计目的是求$x$
- 如果 $x$ 是已知确定的，$θ$ 是变量，这个函数叫做似然函数，最大似然估计目的是求参数$θ$ 
- 对于一个函数 $P(x|θ)$，为什么有两种名字？ 例如，$f(x,y)=x^y$, 即$x$的$y$次方。如果$x$是已知确定的，例如$f(y)=2^y$, 这是指数函数。 如果y是已知确定的，例如$f(x)=x^2$，这是二次函数。同一个数学形式，从不同的变量角度观察，可以有不同的名字。

##### 3、问题3：最大后验概率估计和最大似然估计是一回事吗？

- 最大似然估计是求参数$θ$, 使似然函数$P(x0|θ)$最大。最大后验概率估计则是想求$θ$使$P(x0|θ)P(θ)$最大



#### 五、词义消歧

##### 0、什么是词义消歧

- 词义消歧(WSD)是一个自然语言处理和本体论的开放问题。歧义与消歧是自然语言理解中最核心的问题，在词义、句义、篇章含义层次都会出现语言根据上下文语义不同的现象，消歧即指根据上下文确定对象语义的过程。**词义消歧即在词语层次上的语义消歧**。

- 语义可以简单地看作是数据所对应的现实世界中的事物所代表的概念的含义，以及这些含义之间的关系，是**数据在某个领域上的解释和逻辑表示**。

-  词性标注与词义消歧是相互关联的两个问题，在语言使用者身上它们往往同时能得到满足。但是目前的计算机系统一般并不能让二者共用参数并同时输出。语义理解包括分词、词性标注、词义消歧、句法解析、语义解析等。它们并不是前馈的，是相互依赖并存在反馈的。词性标注与语义消歧都要依赖上下文来标注，但是词性标注比语义消歧处理起来要更简单，最终结果也往往较好。主要原因是词性标注的标注集合是确定的，而语义消歧并没有，并且量级上词性标注要大得多；词性标注的上下文依赖比语义消歧要短。

- 举例说明

  - 川大学生上网成瘾如患绝症。歧义在于“川大学生”——四川大学的学生；四川的大学生。
  - 两代教授，人格不同。歧义：“两代”——两位代理教授；两个时代的教授。
  - 被控私分国有资产，专家总经理成了被告人。歧义：“专家总经理”——专家和总经理；有专家身份的总经理。
  - 新生市场苦熬淡季。歧义：“新生”——新学生的市场；新产生的市场。

  ​

##### 1、案例描述

​	备注：此案例来自大创项目

​	如果一个句子中，某个词存在歧义，例如“小明参加活动”，“小明，快出来活动一下”，在不同的语境下，“活动”具有不同词性，有时可以作为名词，有时也可作为动词，因此根据上下文，某个词可能会存在多个语义。

​	我们要做的是：输入某一个句子，系统先进行分词处理，然后筛选出具有多个语义的分词。对某一个分词的某一个语义而言，利用语料库文件，并通过贝叶斯公式（模型），提取该分词的上下文信息（即左右分词），计算在该分词下，该语义的先验概率。再计算该语义下该分词出现的概率，并将其作为似然函数。最后在该词的语义样本空间中，根据最大似然估计和先验概率，得到最大后验概率估计，最后得到该分词的最终语义。



##### 2、实验步骤

​	这里用同义词词林作为数据集（里面的数据好像和同义词词林扩展版有点出入，我老师这么叫的，那就这么叫吧），每一条数据格式如下: "#活动/钻营\_贿赂\_受贿(Hn09) 人造\_自动(Eb34) 固定\_灵活\_生动\_平板(Ed14) 移动\_动弹\_跳动\_摇摆\_颤动(Id14) 动作\_举动\_行为(Di20)"，里面的编号对应一个语义类，语义类之间存在层级关系。

​	至于这种层级关系意味着什么，可以做什么，我也没向老师多问，当时就想计算概率吗，按部就班就行，没必要想这么高深的问题。但我还是查了查：

​	*随着级别的递增，词义刻画越来越细，到了第五层，每个分类里词语数量已经不大，很多只有一个词语，已经不可再分，可以称为原子词群、原子类或原子节点。不同级别的分类结果可以为自然语言处理提供不同的服务，例如第四层的分类和第五层的分类在信息检索、文本分类、自动问答等研究领域得到应用。有研究证明，对词义进行有效扩展，或者对关键词做同义词替换可以明显改善信息检索、文本分类和自动问答系统的性能。* 
​	接下来介绍如何利用贝叶斯实现消歧的

​	记**事件**$C=“活动”$ ，**语义类样本空间**$S$为$｛Hn09，Eb34，Ed14，Id14，Di20｝$，利用贝叶斯公式得到的**似然函数**为：$\frac{P(C|Si)}{P(C)}$ ，及**统计在$Si$语义类下出现该词的句子的个数**，这里我们用到了语料库文件（里面全是句子+语义编码），每一个句子格式如下：“迈向/Fb01 充满/Jd06 希望/Df08 的/Kd01 新/Eb28 世纪/Ca18 ——/-1 一九九八年/-1 新年/Ca25 讲话/Dk11 （/-1 附/-1 图片/Dk18 １/-1 张/Dn08 ）/-1”。至于似然函数的**分母**$P(C)$ ，及该词在句子中出现的次数。**在样本数据一定的情况下，该值是个已知值**，对于概率计算的最终结果来说，不存在影响，可不计算。

​	接下来计算**先验概率**$P(Si)$。这里采用**提取该分词的左右分词**的方法，到语料库中计算，**在语义为$Si$的情况下，出现该分词绑定左右分词的句子的个数**，例如：“小明出来活动一下”，“活动“的左分词是“出来”，右分词是“一下”，假设要计算语义类为${Hn09}$ 的先验概率，统计在${Hn09}$的前提下， "出来 + 活动" 或 "活动 + 一下"在语料库中出现的句子次数。如果先验概率不存在，则为1。

​	最后在所有计算的语义类的后验概率中，找到最大的后验概率估计，对应的语义类就是词义消歧的结果。



##### 3、效果演示

![词义消歧](E:\java面试\Study_Repositories\images\机器学习\词义消歧.gif)



#### 六、参考文档

1、[详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解](https://blog.csdn.net/u011508640/article/details/72815981)

2、[先验概率、后验概率与似然估计--通俗易懂的解释](https://blog.csdn.net/Mr_HHH/article/details/83346428)

3、[【自然语言处理】论述自然语言处理的技术范畴](https://blog.csdn.net/jiajikang_jjk/article/details/83620057)

4、[贝叶斯公式的直观理解](https://www.cnblogs.com/yemanxiaozu/p/7680761.html)

5、[【机器学习】贝叶斯分类简述到朴素贝叶斯分类器](https://blog.csdn.net/Jiajikang_jjk/article/details/100151700)

6、[条件概率/全概率/贝叶斯公式](https://blog.csdn.net/qq_31073871/article/details/81077386)

7、[同义词词林扩展版](https://blog.csdn.net/sinat_33741547/article/details/80016713)

